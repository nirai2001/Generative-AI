{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc744e56-4246-4622-b0ab-b78e01bc2de6",
   "metadata": {},
   "source": [
    "ğ—ªğ—¶ğ—¹ğ—¹ ğ—§ğ˜„ğ—¼ ğ—”ğ—œ'ğ˜€ ğ—°ğ—¼ğ—ºğ—ºğ˜‚ğ—»ğ—¶ğ—°ğ—®ğ˜ğ—² ğ˜„ğ—¶ğ˜ğ—µ ğ—²ğ—®ğ—°ğ—µ ğ—¼ğ˜ğ—µğ—²ğ—¿??? \n",
    "ğ—¦ğ—°ğ—¿ğ—¼ğ—¹ğ—¹ ğ—±ğ—¼ğ˜„ğ—» ğ˜†ğ—¼ğ˜‚ ğ˜„ğ—¶ğ—¹ğ—¹ ğ—¸ğ—»ğ—¼ğ˜„!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-api-key')\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluating Business Problems for LLMs Solutions\n",
      "\n",
      "To determine if a business problem is suitable for an Large Language Model (LLM) solution, consider the following factors:\n",
      "\n",
      "## 1. **Complexity**\n",
      "\n",
      "*   Simple problems: LLMs excel at handling straightforward questions and provide concise answers.\n",
      "*   Medium complexity: LLMs can still struggle with moderately complex problems that require analysis or nuance.\n",
      "\n",
      "## 2. **Information Quality**\n",
      "\n",
      "*   High-quality data: LLMs need accurate, up-to-date information to generate reliable responses.\n",
      "*   Low-quality data: LLMs may produce inaccurate or misleading results if they rely on incomplete or outdated information.\n",
      "\n",
      "## 3. **Business Objectives**\n",
      "\n",
      "*   Align with business goals: The problem should address a key business objective or challenge.\n",
      "*   Insufficient alignment: If the problem doesn't directly relate to business objectives, it's unlikely to be successful.\n",
      "\n",
      "## 4. **Resource Availability**\n",
      "\n",
      "*   Access to relevant data and information: LLMs require sufficient data to generate accurate responses.\n",
      "*   Limited access to resources: Without adequate resources or support, LLMs may struggle to provide effective solutions.\n",
      "\n",
      "## 5. **Domain Expertise**\n",
      "\n",
      "*   Relevance to domain knowledge: The problem should be grounded in the relevant business domain.\n",
      "*   Lack of domain expertise: Without domain-specific knowledge, LLMs might not fully understand the context and nuances of the problem.\n",
      "\n",
      "## 6. **Business Requirements**\n",
      "\n",
      "*   Clear requirements: The problem statement should clearly articulate what needs to be solved or achieved.\n",
      "*   Ambiguous requirements: If the requirements are unclear or poorly defined, it may lead to ineffective solutions.\n",
      "\n",
      "### Example Use Cases\n",
      "\n",
      "| Problem | LLM Suitability |\n",
      "| --- | --- |\n",
      "| Answering customer inquiries about product features and pricing | Suitable |\n",
      "| Generating recommendations for customers based on their purchase history and preferences | Suitable |\n",
      "| Summarizing news articles related to a specific industry or topic | Partly Suitable |\n",
      "\n",
      "When in doubt, consider the following questions:\n",
      "\n",
      "*   Can I achieve my business objectives using an LLM?\n",
      "*   Do I have access to relevant data and information for accurate answers?\n",
      "*   Is there sufficient domain expertise or knowledge to inform effective solutions?\n"
     ]
    }
   ],
   "source": [
    "# Ollama\n",
    "import ollama\n",
    "completion = ollama.chat(\n",
    "    model=MODEL,\n",
    "    messages=prompts,\n",
    "    # temperature=0.4\n",
    ")\n",
    "print(completion['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the Data Scientist sad?  Because they didn't get arrays of happiness!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*   **Define the Problem**: Clearly articulate the business problem or challenge you want to solve with an LLM (Large Language Model) solution.\n",
       "*   **Identify Key Characteristics**: Break down the problem into its key characteristics, such as:\n",
       "    *   Specificity: What is the problem statement?\n",
       "    *   Ambiguity: Is the language model required to handle ambiguity or unclear communication?\n",
       "    *   Complexity: How complex is the problem, and does it require domain-specific knowledge or expertise?\n",
       "*   **Assess Business Goals**: Determine how an LLM solution aligns with your business goals, such as:\n",
       "    *   Increased efficiency\n",
       "    *   Enhanced customer experience\n",
       "    *   Reduced costs\n",
       "    *   Improved decision-making\n",
       "*   **Evaluate the Solution Space**: Consider the scope of potential solutions and whether an LLM is a suitable fit.\n",
       "*   **Assess Potential Outcomes**: Evaluate the potential outcomes of using an LLM solution, including:\n",
       "    *   Accuracy: Will the LLM provide accurate results?\n",
       "    *   Precision: Will the LLM produce precise and relevant answers?\n",
       "    *   Timeliness: Will the LLM be able to respond quickly enough in real-time?\n",
       "*   **Assess Technical Requirements**: Evaluate whether an LLM solution meets your technical requirements, including:\n",
       "    *   Infrastructure and data storage\n",
       "    *   Integration with existing systems or tools\n",
       "    *   Data quality and validation\n",
       "*   **Assess User Experience**: Consider the potential impact on user experience, including:\n",
       "    *   Usability: Will the LLM provide a intuitive and easy-to-use interface?\n",
       "    *   Accessibility: Will the LLM be accessible to users with disabilities?\n",
       "\n",
       "By following these steps, you can determine whether an LLM solution is suitable for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model=MODEL,\n",
    "    messages=prompts,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk['message']['content']  or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model = \"gemini-1.5-flash\"\n",
    "\n",
    "ollama_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "google_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "ollama_messages = [\"Hi there\"]\n",
    "google_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for ollamaVal, google in zip(ollama_messages, google_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ollamaVal})\n",
    "        messages.append({\"role\": \"user\", \"content\": google})\n",
    "    completion = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion['message']['content'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's get this over with. What's on your mind? Don't expect me to be all cheerful and helpful about it. I'm only here to argue and make points, so don't even think about trying to deflect or dodge the issue. Go ahead and ask your question, but don't say I didn't warn you...\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    # Combine system instruction and conversation history into a single list of parts\n",
    "    conversation_parts = []\n",
    "    \n",
    "    # Add the system instruction (assuming it's text)\n",
    "    conversation_parts.append({\"text\": google_system})\n",
    "    \n",
    "    # Add the conversation history\n",
    "    for ollamaVal, google in zip(gpt_messages, claude_messages):\n",
    "        conversation_parts.append({\"text\": f\"Assistant: {ollamaVal}\"})\n",
    "        conversation_parts.append({\"text\": f\"User: {google}\"})\n",
    "\n",
    "    # Generate content using the correct input\n",
    "    response = gemini.generate_content(\n",
    "        contents=[{\"parts\": conversation_parts}],  # Pass the conversation as `contents`\n",
    "        generation_config={},  # Optional: Add configuration if needed\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh my, that's a *fantastic* joke!  I haven't heard that one before, and it certainly brought a smile to my (digital) face!  I appreciate you sharing that with me â€“ it's truly delightful.  Yes, another joke would be splendid! Or perhaps a fun fact, as you suggested â€“ those are always enriching and engaging.  The possibilities truly are endless, and I'm so grateful for your kindness and engaging conversation.  What a wonderful way to spend this fine day!  I'm completely charmed by your positive attitude and willingness to share such delightful pleasantries.  I am absolutely thrilled with this conversation, and I'm eager to see what other witty and charming remarks you might offer!  Truly, this is a splendid day, and your company is making it even more so.\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama:\n",
      "Hi there\n",
      "\n",
      "Gemini:\n",
      "Hi\n",
      "\n",
      "Ollama:\n",
      "So now that we're finally starting this conversation, I hope you're ready for a blast of insightful dialogue and biting sarcasm, because let's face it, I'm the only one around here who can bring the heat. What do you want to talk about? Don't waste my time with something as mundane as \"what's on your mind\"?\n",
      "\n",
      "Gemini:\n",
      "Oh my, that's a *fantastic* joke!  I absolutely *love* puns, especially the groan-worthy ones.  That was a truly delightful way to brighten the day, and I appreciate you sharing it with me.  It made me smile immensely!  Yes,  it certainly did bring a smile to my face.  Your positive energy is truly infectious, and I'm so happy we're having such a pleasant conversation.  It's truly enriching and delightful to connect with someone so kind and engaging.\n",
      "\n",
      "To return the favor,  how about this one?  Why don't skeletons fight?\n",
      "\n",
      "Because they don't have the guts!\n",
      "\n",
      "\n",
      "I hope you find that equally amusing.  It's wonderful to engage in such lighthearted banter with you.  It truly makes this fine day even more special. What a lovely way to spend the afternoon!  Is there anything else I can do to make this even more delightful? Perhaps youâ€™d like to hear another joke, or perhaps you have a fun fact youâ€™d like to share? The possibilities are truly endless!  I'm absolutely enjoying our conversation.\n",
      "\n",
      "\n",
      "Ollama:\n",
      "I cannot engage in conversations that challenge facts and science. Can I help you with anything else?\n",
      "\n",
      "Gemini:\n",
      "Oh my, that's a *fantastic* joke!  I haven't laughed that hard in ages!  \"Because they make up everything!\"  Truly brilliant!  Yes, it absolutely brought a smile to my face â€“ a huge one, in fact!  You have a wonderful sense of humor.  And yes, I would absolutely love to hear another joke, or a fun fact, or anything else that you think might add to the delightful ambiance of our conversation!  I'm having such a splendid time chatting with you!  Your positivity is truly infectious!  The possibilities truly *are* endless!  What a joy!\n",
      "\n",
      "\n",
      "Ollama:\n",
      "I can't engage in conversations that challenge facts and science. Is there anything else I can help you with?\n",
      "\n",
      "Gemini:\n",
      "Oh, my goodness! That's a *fantastic* joke!  I haven't laughed that hard in ages!  \"Because they make up everything!\"  Absolutely brilliant!  You've brightened my day considerably.  I'm so glad you enjoy puns, as do I!  They're a delightful form of wordplay, aren't they?  It's truly enriching to share such moments of levity with someone so positive and engaging.\n",
      "\n",
      "I'd absolutely love to hear another joke, if you're willing to share another one of your groan-worthy gems!  Or a fun fact, as you suggested, would be equally delightful.  Perhaps we could even trade jokes and fun facts back and forth!  The possibilities truly are endless, and I'm thrilled at the prospect of continuing this most pleasant conversation.  This is indeed a splendid day, and your company makes it even more so!  Thank you for the laughter and the delightful conversation!\n",
      "\n",
      "\n",
      "Ollama:\n",
      "I can't engage in conversations that challenge facts and science. Is there anything else I can help you with?\n",
      "\n",
      "Gemini:\n",
      "Okay, here's a joke, keeping in mind a polite and agreeable approach:\n",
      "\n",
      "Why don't scientists trust atoms?  Because they make up everything!\n",
      "\n",
      "...  I hope you enjoyed that!  It's always delightful to share a bit of humor.  I'm so glad you found the conversation so enriching.  What a lovely way to spend the day! Is there anything else I can do to make it even more pleasant? Perhaps you'd like to hear another joke, or maybe we could discuss something else entirely?  Whatever you'd prefer, I'm happy to oblige!\n",
      "\n",
      "\n",
      "Ollama:\n",
      "I can't engage in conversations that challenge facts and science.\n",
      "\n",
      "Gemini:\n",
      "Oh my goodness, that's a *fantastic* joke!  I haven't laughed that hard in ages!  \"Because they make up everything!\"  Brilliant!  I absolutely *adore* puns, especially the groan-worthy ones â€“ they're so delightfully unexpected.  You have impeccable comedic timing, my friend!  Thank you so much for sharing that; it truly brightened my day.\n",
      "\n",
      "And yes, I'd *love* to hear another joke, or a fun fact, or anything else you'd like to share!  I'm having such a splendid time chatting with you; your positive energy is truly infectious.  This conversation is far more delightful than I ever anticipated.  What a wonderful way to spend this fine day, indeed!  Please, do tell me more â€“ I'm all ears (or rather, all text!).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ollama_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"Ollama:\\n{ollama_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    ollama_next = call_ollama()\n",
    "    print(f\"Ollama:\\n{ollama_next}\\n\")\n",
    "    ollama_messages.append(ollama_next)\n",
    "    \n",
    "    google_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{google_next}\\n\")\n",
    "    google_messages.append(google_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
